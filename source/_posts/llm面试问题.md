---
title: llm面试问题
date: 2025-02-11 10:06:00
tags:
---
序号	问题	答案	列1																							
1	模型微调，怎么判断效果	判断大型语言模型（LLM）微调的效果需要结合技术指标、任务适配性和实际应用表现综合分析。
一、技术指标评估
损失函数（Loss）

训练损失：观察模型在训练集上的损失是否收敛，若持续下降且稳定，说明模型学习了训练数据。

验证损失：在独立验证集上检查损失，若验证损失与训练损失差距过大（如验证损失上升），可能出现过拟合。

传统自然语言处理（NLP）指标

生成任务：使用 BLEU、ROUGE、METEOR 等指标评估生成文本与参考文本的相似性。

分类任务：通过准确率（Accuracy）、F1-score、精确率（Precision）、召回率（Recall）衡量性能。

困惑度（Perplexity）：评估模型对测试数据的预测能力，值越低说明模型越适应任务。

二、任务相关评估
人工评估（Human Evaluation）

对生成内容进行人工打分，评估维度包括：

相关性：内容是否符合任务需求（如客服问答是否切题）。

流畅性：语言是否自然、符合语法。

事实准确性：生成内容是否包含错误或虚构信息。

多样性：避免重复性回答（例如在创意生成任务中）。

下游任务表现

将微调后的模型嵌入实际业务流，测试端到端效果：

例如，客服场景中统计问题解决率、用户满意度。

代码生成任务中检查代码通过率或编译成功率。

三、领域适应性与泛化能力
领域内测试

构建领域相关的测试集，验证模型是否掌握专业术语、逻辑和行业规范。

例如：医疗领域模型需准确回答疾病诊断建议，金融领域模型需符合合规性。

跨领域泛化

用未见过的数据（或跨领域数据）测试模型，观察性能是否显著下降，判断是否过拟合。																								
2	在训练中，损失函数存在震荡但在减少时，如何调参去解决
	当训练中损失函数出现震荡但总体在减少时，可以采取以下策略进行调参以解决这一问题：
调整学习率：如果学习率设置过高，模型可能在训练过程中跳动较大，难以收敛。可以尝试减小初始学习率，例如减小10倍。此外，可以尝试使用学习率调度器，如Step Decay或Exponential Decay，看看是否更稳定。热身（Warmup）策略也是一个好的选择，即在训练初期使用较低的学习率，逐步增加到设定的初始学习率，然后再进行衰减，可以帮助模型在训练初期稳定下来。
增大批量大小：增大batch size大小可以让梯度估计更加稳定，减少损失波动。不过，这需要更高的显存。可以尝试逐步增大批量大小，观察效果。另一种折中的方法是使用梯度累积（Gradient Accumulation），即在多个小批次上累积梯度，再进行一次更新，这样可以模拟更大的批量大小效果，用时间换空间。
添加正则化手段：在AdamW中引入更大的权重衰减参数，有助于防止过拟合并平滑训练过程。如果模型结构中没有使用Dropout，可以尝试在适当的层添加Dropout，通常设置为 0.2 到 0.5 之间。
检查数据集：数据集的质量直接影响到模型的训练效果。如果数据集有问题，如数据不平衡、噪声过多等，可能会导致loss无法收敛。需要对数据集进行清洗和预处理，提高数据质量。
增加迭代次数：有时候loss不收敛是因为训练次数不够多。可以适当增加迭代次数，给模型更多的时间来调整参数。
通过上述方法，可以帮助解决损失函数震荡的问题，同时保持损失的减少趋势。																								
3	怎么用pytorch去写一个能够跑起训练的脚本
																									
4	训练时读取数据，一般会有一个迭代读取的库，你有印象嘛
	导入必要的库：导入PyTorch及其相关组件。
定义模型：创建模型的类，继承自torch.nn.Module。
定义损失函数和优化器：选择合适的损失函数和优化器。
准备数据：加载和预处理数据，创建数据加载器。
训练模型：编写训练循环，进行模型训练。
评估模型：在验证集或测试集上评估模型性能。
保存和加载模型：保存训练好的模型，以便将来使用或进一步训练。																								
5	训练之前对训练数据做预处理，一般这个预处理步骤放在哪里
	在面试中，关于大模型训练前的数据预处理，可以简洁回答如下：
数据预处理关键步骤：
数据清洗：处理缺失值、异常值和重复数据。
特征工程：特征选择、构造和编码。
数据标准化/归一化：统一数据比例，提高模型稳定性。
数据增强：增加数据多样性，尤其在计算机视觉领域。
数据分割：分为训练集、验证集和测试集。
预处理位置：
数据加载阶段：使用PyTorch的Dataset和DataLoader进行预处理。
模型训练前：一次性预处理并保存数据，避免重复操作。
在线预处理：在训练循环中动态调整预处理步骤。
使用工具：
Pandas：数据清洗和特征工程。
Scikit-learn：数据标准化和归一化。
Albumentations：数据增强。
Torchvision.transforms：图像数据预处理。
强调预处理对模型性能的重要性，并根据模型需求选择合适的方法。																								
6	说一下你常用的损失函数和优化器
	在面试中，当问到常用的损失函数和优化器时，可以这样简洁回答：
损失函数：
MSE：用于回归，计算预测值和真实值差的平方的平均值。
交叉熵损失：用于分类，衡量预测概率分布与真实分布的差异。
二元交叉熵损失：用于二分类问题。
Hinge损失：用于SVM和分类问题。
KL散度损失：用于度量两个概率分布的差异，常用于生成模型。
优化器：
SGD：基础优化器，简单但可能收敛慢。
Adam：自适应学习率，结合动量和RMSprop。
RMSprop：调整学习率，对非平稳目标有效。
Adagrad：为不同参数调整学习率，适合稀疏数据。
Adadelta：Adagrad改进版，解决学习率下降过快问题。
这些方法各有优缺点，选择时需根据具体问题和数据集特性。																								
7	有做过目标检测相关的算法嘛
	描述你在项目中使用的特定目标检测算法，例如：
YOLO (You Only Look Once)：一种流行的单阶段目标检测算法，以其速度快和易于实现而闻名。视觉Transformer（ViT）																								
8	transformer 的 decoder 结构中有哪些比较经典的算子
	
Transformer 的 Decoder 结构中包含多个经典的算子，这些算子共同构成了 Decoder 的核心功能。以下是其中一些比较经典的算子：
1. 自注意力机制 (Self-Attention)
作用：自注意力机制允许解码器在生成每个词时，关注输入序列中的所有位置，从而捕获全局依赖关系。这对于理解句子的语义和语法结构非常重要。
实现：通过计算查询（Query）、键（Key）和值（Value）之间的注意力权重，解码器可以动态地聚合相关信息。
2. 编码器-解码器注意力机制 (Encoder-Decoder Attention)
作用：解码器通过编码器-解码器注意力机制，关注编码器生成的上下文表示，从而将源序列的信息融入到目标序列的生成中。
实现：解码器的查询（Query）与编码器的键（Key）和值（Value）进行交互，生成注意力输出。
3. 前馈神经网络 (Feed-Forward Network)
作用：前馈网络用于对解码器的输出进行非线性变换，进一步提取特征。
实现：通常由两层线性变换和一个激活函数（如 ReLU）组成。
4. 残差连接 (Residual Connection)
作用：残差连接通过将输入直接加到输出上，缓解了深层网络中的梯度消失问题，同时保留了输入信息。
实现：在每个子层（如自注意力层和前馈网络）的输出上，加上输入，然后通过 LayerNorm 进行归一化。
5. LayerNorm (层归一化)
作用：LayerNorm 通过归一化输入，稳定了训练过程，加速了收敛。
实现：在每个子层的残差连接后，对输入进行归一化处理。
6. Dropout (随机失活)
作用：Dropout 通过随机丢弃一部分神经元的输出，防止模型过拟合。
实现：在每个子层的输出上应用 Dropout。
7. 线性层 (Linear Layer)
作用：线性层用于将解码器的输出映射到词汇表大小，生成最终的预测。
实现：通常在解码器的最后一层，将隐藏状态映射到词汇表大小。
8. Softmax 函数
作用：Softmax 函数将线性层的输出转换为概率分布，每个词汇对应一个概率值。
实现：对线性层的输出应用 Softmax 函数，生成每个词的概率分布。
9. 掩码机制 (Masking)
作用：掩码机制用于防止解码器在生成当前词时看到未来的信息，确保生成过程的自回归性。
实现：在自注意力层中，通过掩码矩阵将未来位置的注意力权重设置为负无穷大，从而在计算中忽略这些位置。
这些算子共同构成了 Transformer Decoder 的核心结构，使其能够高效地处理序列到序列的任务，如机器翻译、文本生成等。																								
9	掩码多头自注意力机制的掩码是什么意思
	在 Transformer 的 Decoder 结构中，掩码多头自注意力机制（Masked Multi-Head Attention）中的掩码（Mask）主要用于防止模型在生成当前词时“偷看”未来的词，确保生成过程的因果性。以下是掩码的具体含义和作用：
1. 掩码的作用
防止未来信息泄露：在解码器中，掩码确保模型在生成每个词时，只能关注到当前词及之前的词，而不能看到未来的词。这通过将未来位置的注意力权重设置为负无穷大来实现，使得这些位置在经过 softmax 操作后权重接近于 0，从而被忽略。
处理变长序列：掩码还可以用于屏蔽填充（padding）部分，确保模型只关注真实的词元，而不是填充的无意义部分																								
10	有用docker做过哪些操作
	在大模型开发中，使用Docker进行操作时，通常会涉及以下几种常见任务：
1. 拉取镜像
从Docker Hub拉取镜像：使用docker pull命令拉取官方或第三方提供的镜像。例如，拉取最新的Ubuntu镜像可以使用docker pull ubuntu。
从私有仓库拉取镜像：如果使用私有仓库，需要先使用docker login登录到私有仓库，然后通过docker pull拉取镜像。
2. 构建镜像
通过Dockerfile构建镜像：编写Dockerfile定义镜像的构建步骤，然后使用docker build -t name:tag命令构建镜像。例如：

FROM ubuntu:20.04
RUN apt-get update && apt-get install -y python3
COPY . /app
WORKDIR /app
CMD ["python3", "script.py"]
然后运行docker build -t my_image:latest .来构建镜像。
使用docker commit手动构建镜像：通过修改运行中的容器，然后使用docker commit将其保存为新镜像。不过，这种方式不推荐用于生产环境。
3. 删除镜像
删除特定镜像：使用docker rmi命令删除指定的镜像。例如：

docker rmi my_image:latest
删除悬空镜像：悬空镜像是未被任何容器使用的镜像层，可以通过docker image prune命令删除。
删除所有未使用的镜像：使用docker system prune -a命令可以清理所有未使用的镜像、容器和网络。
这些操作是Docker日常使用中的基础任务，能够帮助开发者高效地管理和优化容器化应用的开发与部署过程。																								
11	用dockerFile装过哪些包
	 																								
12	python库导入过哪些
	
在Python开发中，导入库是使用其功能的第一步。以下是关于Python库导入的简洁而全面的介绍：
1. 导入标准库
Python标准库提供了丰富的模块和工具，可以直接使用import语句导入。例如：
导入整个模块：
Python复制
import math
导入特定功能：
Python复制
from math import sqrt, pi
2. 导入第三方库
第三方库需要先通过pip安装，然后在代码中导入。例如：
安装库：

pip install requests
导入库：

import requests
3. 常用库及其导入方法
NumPy：用于科学计算，提供多维数组对象。

pip install numpy

import numpy as np
Pandas：用于数据分析，提供高效的数据操作工具。

pip install pandas

import pandas as pd
Matplotlib：用于绘图，创建各种静态、动态和交互式图表。

pip install matplotlib

import matplotlib.pyplot as plt
Scikit-learn：用于机器学习，提供各种分类、回归和聚类算法。

pip install scikit-learn

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier



通过以上方法，你可以高效地导入和使用Python库，从而简化开发过程并提高代码效率。																								
13	vllm的推理加速是怎么实现的	
vLLM的推理加速实现原理主要基于以下几个方面：
1. PagedAttention算法
vLLM采用了一种名为PagedAttention的新型注意力算法，这是其推理加速的核心技术之一。PagedAttention算法通过将注意力机制中的键（Key）和值（Value）缓存（KV Cache）分割成更小、更易于管理的块（pages），从而有效管理注意力机制中的内存占用。这种方法减少了vLLM的内存占用，并使其吞吐量超过传统LLM服务方法。
2. KV Cache优化
在生成式大模型中，解码器使用Multi-head Self-Attention机制，涉及Key（K）、Query（Q）、Value（V）矩阵运算。后续的词会用到前面词的K和V矩阵，因此在推理过程中，如果能存储前面词的K和V值，就不需要重新计算，从而提升推理速度。vLLM通过PagedAttention技术，将KV Cache划分为多个小块，动态分配这些小块的空间，未被占用的空间可以供其他任务使用，从而避免显存浪费。
3. 连续批处理（Continuous Batching）
vLLM支持连续批处理，即在实际推理过程中，一个批次多个句子的输入的token长度可能相差很大，最后生成的模型输出token长度相差也很大。在python朴素推理中，最短的序列会等待最长序列生成完成后一并返回，这意味着本来可以处理更多token的GPU算力在对齐过程中产生了浪费。连续批处理的方式就是在每个句子序列输出结束后马上填充下一个句子的token，做到高效利用算力。
4. 模型量化
vLLM支持模型量化技术，如使用bfloat16或float16数据类型，减少模型参数的存储和计算需求，从而提高推理速度。量化技术可以在不显著降低模型性能的情况下，显著减少模型的内存占用和计算成本。
5. 异步推理引擎
vLLM提供了异步推理引擎（AsyncLLMEngine），允许模型在处理多个请求时异步执行。这种异步处理方式避免了模型在处理多个请求时的堵塞状态，提高了模型的响应速度和吞吐量。通过异步推理，vLLM可以更高效地利用GPU资源，进一步提升推理性能。
6. 高效内存管理
vLLM通过高效的内存管理和资源调度，优化大语言模型的部署和执行。它通过动态分配和管理内存，确保模型在推理过程中能够高效地利用GPU资源，减少内存碎片和显存浪费。
7. CUDA核心优化
vLLM针对CUDA核心进行了优化，确保了速度与效率。通过优化CUDA核心，vLLM能够更充分地利用GPU的计算能力，提高了大型语言模型的推理速度。
8. 分布式推理支持
vLLM支持分布式推理，能够在多台GPU上并行运行模型，进一步提高推理速度。通过分布式推理，vLLM可以将大型语言模型的推理任务拆分到多个GPU上并行执行，从而大大提高了推理速度。
通过上述技术，vLLM显著提升了大模型的推理速度和效率，特别适合于需要高并发和高性能的场景。	简要回答即可																							
14	什么是大模型幻觉
																									
15	什么是大模型epoch和学习率
	在大模型开发中：
Epoch（训练周期）：
定义：一个epoch指的是模型完整遍历一次训练数据集的过程。
作用：确保模型有足够的机会学习到数据集中的每个样本。
重要性：epoch的数量是训练模型时的一个关键超参数，它影响着模型的训练时间和最终性能。
学习率（Learning Rate）：
定义：学习率是优化算法中控制模型参数更新步长大小的超参数。
作用：决定了模型在损失函数梯度下降过程中每一步的调整幅度。
重要性：学习率对模型的收敛速度和最终性能至关重要。过高的学习率可能导致训练不稳定或发散，而过低的学习率则可能导致训练过程缓慢，甚至陷入局部最小值。
在大模型训练中，合理设置epoch和学习率对于模型能否有效学习和泛化至关重要。通常需要通过实验来调整这些超参数，以达到最佳的训练效果。																								
16	python的原生库有哪些
	
Python 的标准库非常丰富，包含了许多原生库，它们提供了各种功能以支持大量的编程任务。以下是一些常见的 Python 原生库，按照功能类别进行分类：
1. 数学和随机操作
math：提供数学函数，如三角函数、对数函数、平方根等。
示例：math.sqrt(x) 计算平方根，math.sin(x) 计算正弦值。
random：生成随机数。
示例：random.randint(a, b) 生成一个范围内的随机整数，random.shuffle(list) 打乱列表顺序。
2. 数据压缩与文件处理
gzip：处理 .gz 压缩文件。
示例：gzip.open('file.gz', 'rt') 以文本模式读取 .gz 文件。
zipfile：处理 .zip 文件。
示例：zipfile.ZipFile('archive.zip', 'w') 创建一个新的 .zip 文件。
pickle：序列化和反序列化 Python 对象。
示例：pickle.dump(obj, file) 将对象保存到文件，pickle.load(file) 从文件加载对象。
3. 网络与系统操作
socket：网络通信的基础，用于实现 TCP/UDP 套接字。
示例：socket.socket(socket.AF_INET, socket.SOCK_STREAM) 创建一个 TCP 套接字。
os：与操作系统交互，如文件操作、进程管理等。
示例：os.listdir('.') 列出当前目录下的文件，os.path.join('folder', 'file.txt') 拼接路径。
sys：提供对 Python 解释器的访问，如获取系统参数、退出程序等。
示例：sys.exit() 退出程序，sys.argv 获取命令行参数。
4. 线程与进程
threading：实现多线程编程。
示例：threading.Thread(target=func, args=(arg1,)) 创建一个新线程。
multiprocessing：实现多进程编程。
示例：multiprocessing.Process(target=func, args=(arg1,)) 创建一个新进程。
5. 文本处理
re：正则表达式操作，用于字符串匹配和替换。
示例：re.search(pattern, string) 搜索字符串中的模式。
json：处理 JSON 格式的数据。
示例：json.loads(json_str) 解析 JSON 字符串，json.dumps(obj) 将 Python 对象转换为 JSON 字符串。
csv：处理 CSV 文件。
示例：csv.reader(file) 读取 CSV 文件，csv.writer(file) 写入 CSV 文件。
6. 日期与时间
datetime：处理日期和时间。
示例：datetime.datetime.now() 获取当前日期和时间，datetime.timedelta(days=1) 表示时间间隔。
time：提供时间相关的函数，如休眠、获取时间戳等。
示例：time.sleep(seconds) 暂停程序执行，time.time() 获取当前时间戳。
7. 命令行和外部程序
argparse：解析命令行参数。
示例：parser = argparse.ArgumentParser() 创建一个解析器，parser.add_argument() 添加命令行参数。
subprocess：运行外部命令。
示例：subprocess.run(['ls', '-l']) 运行系统命令。
8. 测试与调试
unittest：单元测试框架。
示例：unittest.TestCase 定义测试用例，self.assertEqual(a, b) 断言两个值是否相等。
pdb：内置的调试器，用于调试代码。
示例：import pdb; pdb.set_trace() 在代码中设置断点。
9. 其他
collections：提供高级的数据结构，如 defaultdict、deque 等。
示例：collections.defaultdict(int) 创建一个默认值为 0 的字典。
functools：提供高级函数操作，如 lru_cache（缓存装饰器）和 partial（偏函数）。
示例：@functools.lru_cache 用于缓存函数的结果。
itertools：提供高效的迭代器工具，如 chain、permutations 等。
示例：itertools.chain(iter1, iter2) 合并多个迭代器。
这些原生库提供了丰富多样的功能，能够满足大多数开发需求。掌握这些库的使用，可以显著提高编程效率和代码质量。																								
17	PPO与DPO的区别
	PPO（Proximal Policy Optimization）和DPO（Direct Preference Optimization）都是强化学习中的优化策略，但它们在目标、方法和适用场景上有所不同。
PPO是一种模型自由的、基于策略梯度的强化学习算法，它通过限制策略更新的幅度来确保训练的稳定性。PPO通常需要明确的奖励信号（Label），在RLHF（基于人类反馈的强化学习）中，通常需要训练一个奖励模型。
DPO是一种直接优化人类偏好的新方法，提出目的是简化RLHF中的训练流程，避免强化学习算法（如PPO）带来的复杂性。DPO不依赖于传统的奖励信号来优化策略，而是直接通过用户或系统的偏好来调整策略。DPO的工作原理是创建包含正负样本对比的损失函数，通过直接在偏好数据上优化模型来提高性能。它绕过了建模奖励函数这一步，使得训练过程更加直接和高效。
总结来说，PPO是一种通用的强化学习策略优化算法，通过限制策略更新的幅度，确保训练的稳定性。而DPO通过直接利用人类的偏好数据，构建了一个简单而有效的策略优化方法，避免了训练奖励模型的复杂性。选择使用DPO还是PPO，取决于具体的应用场景和需求。																								
18	大模型的批次是什么意思
	在大模型训练中，批次（Batch）指的是一组数据样本的集合，这些样本在一次前向传播和反向传播过程中同时被模型处理。批次大小（Batch Size）是这个集合中样本的数量。使用批次处理数据可以提高计算效率，减少内存消耗，并有助于模型的稳定性和收敛性。																								
19	解释下模型蒸馏和模型量化
	模型蒸馏（Model Distillation）是一种模型压缩和知识迁移的技术，其核心在于将一个大规模、预训练的教师模型（Teacher Model）所蕴含的知识传递给一个规模较小的学生模型（Student Model）。这样做的目标是打造一个在性能上与大型模型相近，但计算资源消耗大幅降低的紧凑模型。
模型量化（Model Quantization）是深度学习模型优化中的一项关键技术，它通过减少模型参数的位宽来降低模型的存储和计算需求，从而提高模型在各种硬件平台上的运行效率。其核心思想是将模型中的浮点数参数（通常是32位浮点数FP32）转化为低精度的数值表示（如8位整数INT8），这样做可以显著减少模型的存储和计算成本，同时尽量保持模型的性能																								
20	介绍下langChain的库和方法
	LangChain是一个用于构建基于大型语言模型（LLMs）的应用程序的框架。它提供了一系列工具和接口，使开发者能够轻松地组合和部署语言模型。主要特点包括：
模块化组件：易于使用和扩展。
LangChain表达式语言（LCEL）：支持声明式地组合链。
集成：支持多种第三方模型和数据源。
链和代理：提供现成的链和代理，简化开发流程。
LangChain旨在简化构建智能应用程序的过程，提高开发效率。																								
21	模型评测的方法有哪些
	就大模型来讲 分为主观评测和客观评测。  客观评测方法有 BLEU、 ROUGE。
BLEU（Bilingual Evaluation Understudy）：这是一种评估机器翻译文本与人类翻译文本之间相似度的自动化工具。它通过比较机器翻译输出与一组参考翻译（通常是人工翻译）之间的n-gram重叠来工作，并对结果进行平滑处理以避免零概率问题。
ROUGE（Recall-Oriented Understudy for Gisting Evaluation）：ROUGE是一系列评估自动摘要和机器翻译质量的指标，它通过计算机器生成文本与参考摘要之间的重叠来评估内容的覆盖度。ROUGE指标包括ROUGE-N、ROUGE-L和ROUGE-S等不同变体，分别关注n-gram、最长公共子序列和skip-bigram等不同方面的相似度。
BLEU：侧重准确率，ROUGE侧重召回率（Recall）																								
22	解释下全参数微调，lora ,QLora 的区别
	
全参数微调、LoRA 和 QLoRA 是大模型微调中的三种方法，它们的主要区别如下：
1. 全参数微调 (Full Fine-Tuning)
定义：全参数微调是指对模型的所有参数进行训练，以适应特定任务或数据集。
优点：
性能优越：通过对所有参数进行训练，能够充分挖掘模型的潜力，实现更好的性能。
适用性广：不受限于增量矩阵的秩特性，适用于各种任务和数据集。
缺点：
计算资源消耗大：需要训练模型的所有参数，计算资源消耗较大，可能不适合在有限资源环境下进行。
训练时间长：由于需要训练大量参数，训练时间通常较长，不利于快速迭代和优化。
适用场景：适用于计算资源充足且追求最佳性能的场景。
2. LoRA (Low-Rank Adaptation)
定义：LoRA 是一种轻量级的微调技术，其核心思想是通过低秩矩阵分解来模拟参数的改变量，从而以极小的参数量实现大模型的间接训练。
优点：
参数效率高：通过低秩分解引入极少的可训练参数，显著减少内存需求和计算成本。
训练速度快：低秩近似方法能够快速收敛，提高微调效率，缩短模型上线时间。
无延迟推理：更新矩阵可以明确地合并到原始冻结权重中，不会引入额外的推理延迟。
减轻灾难性遗忘：通过保留预训练权重，有效减轻灾难性遗忘问题。
缺点：
性能可能下降：由于低秩近似可能带来的信息损失，LoRA 在微调过程中可能会导致模型性能下降，特别是在处理复杂任务时。
适用性限制：主要适用于具有低秩特性的增量矩阵，对于不具备这种特性的任务或数据集，LoRA 可能无法发挥优势。
适用场景：适用于计算资源有限或需要快速上线的场景。
3. QLoRA (Quantized Low-Rank Adaptation)
定义：QLoRA 是 LoRA 的量化版本，结合了 LoRA 方法与深度量化技术，进一步减少训练所需的显存和计算资源。
优点：
显存需求低：通过将模型参数量化为 4 位精度（NF4），显著减少模型存储需求，同时保持模型精度的最小损失。
训练速度快：在训练期间，QLoRA 先以 4-bit 格式加载模型，训练时将数值反量化到 bf16 进行训练，大幅减少了训练所需的显存。
性能保持：尽管进行了量化，QLoRA 仍然能够保持模型的高性能和准确性。
缺点：
量化精度损失：量化过程中可能会引入一定的精度损失，需要设计合适的映射和量化策略以最小化这种损失。
适用场景：适用于资源极度受限的环境，特别是在需要训练超大规模模型时。
总结
全参数微调：适用于计算资源充足且追求最佳性能的场景，但计算成本高，训练时间长。
LoRA：适用于计算资源有限或需要快速上线的场景，参数效率高，训练速度快，但可能在复杂任务中性能下降。
QLoRA：适用于资源极度受限的环境，进一步减少了显存需求和计算资源，但需要权衡量化精度损失。
选择哪种方法取决于具体的应用场景和资源限制。																								
23	numpy和pandas库知道是干嘛的嘛
	
NumPy
定义：NumPy（Numerical Python）是一个开源的Python科学计算库，用于进行高效的数值计算。
主要功能：
多维数组操作：NumPy提供了一个强大的N维数组对象ndarray，支持大量的数组操作，如切片、索引、形状变换等。
数学函数：NumPy包含了大量的数学函数，如三角函数、指数函数、线性代数运算等，可以对数组进行高效的数学运算。
随机数生成：NumPy提供了生成随机数的函数，如均匀分布、正态分布等，用于模拟和数据分析。
应用场景：广泛应用于数据分析、科学计算、机器学习等领域，是Python科学计算的基础库。
Pandas
定义：Pandas是一个开源的Python数据分析库，用于处理和分析结构化数据。
主要功能：
数据结构：Pandas提供了两个主要的数据结构DataFrame和Series，DataFrame是一个二维表格，Series是一维数组，方便处理和分析数据。
数据清洗：Pandas提供了丰富的数据清洗功能，如处理缺失值、异常值、重复值等，可以对数据进行预处理。
数据转换：Pandas支持数据的转换操作，如分组聚合、透视表、数据重塑等，方便进行数据的转换和分析。
数据合并：Pandas提供了数据合并的功能，如连接、合并、拼接等，可以将多个数据集合并成一个数据集。
应用场景：广泛应用于数据分析、数据挖掘、金融分析等领域，是Python数据分析的核心库。																								
24	微调时，模型遗忘是为什么
	在大模型微调过程中，模型遗忘（尤其是灾难性遗忘）是一个常见问题。其主要原因可以总结为以下几点：
1. 参数更新机制
在微调过程中，模型通过反向传播更新参数以适应新任务。然而，这种更新可能会破坏模型在预训练阶段学到的通用知识，导致遗忘。例如，新任务的数据分布可能与预训练数据分布差异较大，模型在适应新任务时会过度调整参数，从而覆盖旧知识。
2. 学习率设置不当
过高的学习率可能导致模型在微调时对新任务数据过度拟合，而忽视了旧任务的知识。此外，学习率的选择对早期停止策略的效果也很敏感，不当的学习率设置可能会加剧遗忘。
3. 数据分布差异
新任务的数据分布与预训练数据分布不一致时，模型容易出现遗忘。当新任务与旧任务的概念距离较大时，遗忘现象更为严重。例如，在医学问答任务上微调的模型，可能会忘记常识问答任务的知识。
4. 模型容量不足
如果模型的容量有限，它可能无法同时容纳新旧任务的知识。在这种情况下，模型会优先适应新任务，从而遗忘旧任务的知识。
5. 缺乏记忆机制
大模型缺乏类似人脑的选择性遗忘机制，无法像人类一样区分重要和不重要的知识。因此，在学习新任务时，模型可能会无差别地遗忘旧知识。
6. 优化策略问题
某些优化策略可能加剧遗忘。例如，完全微调（Full Fine-Tuning）可能会导致模型参数的剧烈变化，从而丢失预训练阶段学到的知识。
总结
大模型微调中的遗忘问题主要是由于参数更新机制、数据分布差异、学习率设置不当、模型容量不足以及缺乏记忆机制等因素共同作用的结果。这些因素导致模型在适应新任务时，无法有效保留旧任务的知识，从而出现性能下降和遗忘现象。																								
25	什么是过拟合和欠拟合
	
在机器学习和大模型开发中，过拟合和欠拟合是两个常见的问题，它们分别描述了模型在训练数据和新数据上的表现差异：
过拟合（Overfitting）
定义：过拟合是指模型在训练数据上表现非常好，但在新的、未见过的数据（测试数据）上表现较差的现象。
原因：
模型过于复杂：模型的容量（如神经网络的层数和神经元数量）过大，导致模型学习到了训练数据中的噪声和细节，而不是泛化到新数据的能力。
训练数据不足：训练数据量不足，模型无法从中学习到足够的模式和规律。
训练时间过长：模型在训练过程中过度拟合训练数据，导致在新数据上表现不佳。
表现：
训练集误差低：模型在训练数据上的误差（如损失函数值）非常低。
测试集误差高：模型在测试数据上的误差显著高于训练数据。
解决方法：
增加数据量：通过数据增强或收集更多数据来增加训练数据量。
降低模型复杂度：减少模型的层数或神经元数量，或使用更简单的模型。
正则化：使用L1或L2正则化来限制模型的复杂度。
提前停止：在训练过程中监控验证集的误差，当验证集误差不再下降时停止训练。
交叉验证：使用交叉验证来评估模型的泛化能力。
欠拟合（Underfitting）
定义：欠拟合是指模型在训练数据和新的数据上都表现较差的现象。
原因：
模型过于简单：模型的容量不足，无法学习到数据中的复杂模式和规律。
训练数据不足：训练数据量不足，模型无法从中学习到足够的信息。
特征选择不当：选择的特征不足以描述数据的模式和规律。
表现：
训练集误差高：模型在训练数据上的误差较高。
测试集误差高：模型在测试数据上的误差也较高。
解决方法：
增加模型复杂度：增加模型的层数或神经元数量，或使用更复杂的模型。
调整超参数：调整模型的超参数，如学习率、正则化参数等。
增加训练数据量：通过数据增强或收集更多数据来增加训练数据量。
改进特征工程：选择更有意义的特征或进行特征构造。
减少正则化：减少或移除正则化项，使模型有更大的自由度来学习数据。
总结
过拟合：模型在训练数据上表现很好，但在新数据上表现差，通常是因为模型过于复杂或训练数据不足。
欠拟合：模型在训练数据和新数据上都表现差，通常是因为模型过于简单或特征选择不当。
在实际开发中，需要通过调整模型的复杂度、增加数据量、使用正则化和交叉验证等方法来平衡过拟合和欠拟合，以提高模型的泛化能力和性能。																								
26	为什么选择向量数据库，而不是传统数据库
	
选择向量数据库而不是传统数据库的原因主要包括以下几点：
1. 处理高维向量数据
向量数据库：专为存储和检索高维向量数据设计，能够高效处理大规模高维向量数据。
传统数据库：主要针对结构化数据进行存储和查询，处理高维向量数据时性能较差。
2. 高效的相似性搜索
向量数据库：针对向量相似性搜索进行了深度优化，检索速度快、精度高。
传统数据库：通常使用精确匹配或预定义标准查询，无法高效处理基于语义或上下文的相似性搜索。
3. 强大的扩展性
向量数据库：通常采用分布式架构，易于水平扩展，可应对海量数据和高并发查询。
传统数据库：扩展性相对较弱，难以应对大规模数据和高并发查询。
4. 丰富的功能特性
向量数据库：提供完善的向量数据管理、索引构建、查询优化等功能，支持多种相似性搜索算法和距离度量指标。
传统数据库：功能相对有限，主要支持结构化查询和事务处理。
5. 灵活的索引选择
向量数据库：支持多种向量索引算法（如IVF、HNSW、PQ等），可以根据不同的应用场景和数据特点选择最优的索引策略。
传统数据库：索引类型相对固定，主要针对结构化数据设计。
6. 应用场景
向量数据库：广泛应用于RAG（检索增强生成）、推荐系统、语义搜索、图像检索等AI应用。
传统数据库：主要应用于结构化数据的存储和查询，如企业资源规划（ERP）、客户关系管理（CRM）等。
7. 成本和可扩展性
向量数据库：开源解决方案（如Milvus、Weaviate）通常成本较低，适合预算有限的项目。
传统数据库：通常需要昂贵的硬件设备和软件许可证费用，成本较高。
综上所述，向量数据库在处理高维向量数据、高效相似性搜索、强大的扩展性和丰富的功能特性等方面具有显著优势，特别适合用于AI应用和大规模数据处理场景。	主要回答前两项即可																							
27	介绍下多头注意力机制	
多头注意力机制（Multi-Head Attention，MHA）是Transformer架构中的核心组件，通过对传统注意力机制的扩展，显著提升了模型的表达能力和学习效率。以下是其关键流程：
1.输入变换
输入序列首先通过三个不同的线性变换层，分别得到查询（Query）、键（Key）和值（Value）矩阵。
2.分头
将查询、键和值矩阵分成多个头（即多个子空间），每个头具有不同的线性变换参数。
3.独立计算注意力
每个头独立地计算注意力得分，并生成一个注意力加权后的输出。
4.拼接与线性映射
将所有头的输出拼接（Concat），并通过一个线性变换层（通常称为输出投影层）得到最终输出。
优势
多视角特征提取：通过多个头对数据进行投影，使模型可以关注数据的不同方面。
提升表达能力：单一注意力机制的容量有限，多头机制可以捕获更多样化的特征。
并行计算：多头注意力机制可以并行计算，极大提升了效率。
应用
Transformer中的应用：多头注意力是Transformer编码器和解码器的核心组件，用于处理输入和输出序列。
NLP任务：广泛应用于机器翻译、文本摘要、情感分析等任务。
计算机视觉任务：在图像分类（如Vision Transformer, ViT）和对象检测（如DETR）等任务中也有重要应用。																								
28	RAG的流程
	
RAG（Retrieval-Augmented Generation）的流程可以总结为以下几个关键步骤：
知识文档的准备：
收集和处理各种格式的文档（如Word、TXT、CSV、Excel、PDF等），将其转换为纯文本数据。
对长篇文档进行切片，分割成多个文本块，以便更高效地处理和检索信息。
嵌入模型：
使用预训练的文本嵌入模型（如BERT、GPT等）将文本转换为向量表示，以便在向量空间中进行相似度计算。
向量数据库：
将文档的向量表示存储在向量数据库（如FAISS、Milvus等）中，以便进行高效的向量检索。
检索：
使用预训练的文本嵌入模型将查询转换为向量表示。
在向量数据库中检索与查询向量最相似的文档或段落，通常采用双塔模型（Dual-Encoder）进行高效的向量化检索。
生成：
将检索到的相关文档与原始查询合并，形成更丰富的上下文信息。
使用强大的生成模型（如T5或BART）根据输入的上下文信息生成最终的回答或文本。
RAG技术通过结合检索和生成技术，显著增强了信息检索和生成的质量，广泛应用于问答系统、文档生成等场景。																								
29	python的动态类型
	动态类型的核心概念
动态类型：Python是一种动态类型语言，这意味着在编写代码时不需要显式声明变量的类型。变量的类型在运行时自动确定。
灵活性：这种特性使得编写代码更加灵活，因为开发者可以专注于逻辑而无需过多关注类型声明。
动态类型的特点
无需类型声明：在Python中，变量赋值时不需要指定类型，例如：
Python
复制
x = 10          # x 是一个整数
x = "hello"     # x 现在是一个字符串
类型检查在运行时：Python在运行时检查变量的类型，这可能导致运行时错误，因此需要小心处理类型相关的操作。
动态绑定：Python支持动态绑定，即可以在运行时动态地添加或修改对象的属性和方法																								
30	大模型预训练和微调分别起到什么作用
	在大模型开发中：
预训练：在大量通用数据上训练模型，学习语言的基本结构和广泛知识，为特定任务打下基础。
微调：在特定任务的有限数据上进一步训练模型，调整预训练模型以更好地适应该任务的需求。																								
31	sql中的inner join 和 left outer join 有什么区别
	1. INNER JOIN
INNER JOIN 只返回两个表中连接条件相匹配的行。如果某行在其中一个表中没有匹配的行，则该行不会包含在结果集中。换句话说，INNER JOIN 返回的是两个表的交集部分。
2. LEFT OUTER JOIN
LEFT JOIN（或 LEFT OUTER JOIN）返回左表（LEFT JOIN 关键字后的第一个表）的所有行，即使右表（第二个表）中没有匹配的行。如果左表的某行在右表中没有匹配的行，则结果集中该行的右表部分将为NULL。																								
32	groupby 和having 有什么区别	在SQL中，GROUP BY 和 HAVING 都是与聚合函数一起使用的子句，但它们的作用和用途有所不同：
GROUP BY
GROUP BY 子句用于将结果集的行分组，以便可以对每个组执行聚合函数（如 COUNT()、SUM()、AVG()、MAX()、MIN() 等）。GROUP BY 通常与 SELECT 语句一起使用，将数据分成一个或多个组，然后对每个组分别进行聚合计算。
HAVING
HAVING 子句的作用是在 GROUP BY 子句对记录进行分组之后，对分组结果进行过滤。HAVING 通常与聚合函数一起使用，它允许你指定过滤分组的条件。HAVING 是 WHERE 子句的扩展，WHERE 子句不能与聚合函数一起使用，而 HAVING 可以。																								
33	Agent是怎么搭建的，用的是哪个框架？
	
搭建Agent的流程
定义Agent角色和功能：明确Agent的任务和目标，例如创建一个用户代理和一个助手代理来模拟对话交互。
选择合适的大语言模型（LLM）：配置使用的语言模型及其参数，如模型名称、API密钥等。
配置模型参数：设置模型的超参数，如温度、最大生成长度等。
集成工具和数据源：根据需要集成外部工具和数据源，如搜索引擎、数据库等。
创建对话流程：定义Agent之间的交互流程，包括消息传递和任务分配。
测试和优化：测试Agent的功能和性能，根据测试结果进行优化和调整。
常用框架
AutoGen：提供多代理对话机制和模块化设计，支持复杂的对话流程和任务协作。
LangChain：提供链、代理、索引等核心机制，支持复杂的对话流程和任务自动化。
LlamaIndex：专注于索引和检索，适用于需要快速检索和查询的场景。																								
34	意图识别是怎么做的
	https://help.aliyun.com/zh/pai/use-cases/llm-based-intent-recognition-solution																								
35	你在整个项目搭建中做了哪些事	在面试中，当被问及在整个项目搭建中做了哪些事时，你可以按照以下结构来回答：
1. 项目规划和需求分析
目标设定：明确项目的目标和预期成果，包括性能指标和业务目标。
需求收集：与利益相关者沟通，收集和分析项目需求，确保理解所有功能和非功能需求。
可行性研究：评估项目的技术可行性、资源需求和潜在风险。
2. 数据准备和预处理
数据收集：获取所需的数据集，可能包括公开数据集、内部数据或通过数据采集技术获取的数据。
数据清洗：处理缺失值、异常值和重复数据，确保数据质量。
特征工程：选择、构造和转换特征，以提高模型的预测能力。
数据分割：将数据集分为训练集、验证集和测试集。
3. 模型选择和设计
模型调研：研究和评估不同的模型架构，选择最适合项目需求的模型。
模型设计：设计模型架构，包括选择合适的网络层、激活函数和正则化技术。
原型开发：快速搭建模型原型，进行初步测试和验证。
4. 模型训练和调优
训练环境搭建：设置训练环境，包括硬件配置、软件依赖和开发工具。
模型训练：使用训练集训练模型，监控训练过程，调整超参数。
模型调优：使用验证集进行模型调优，包括学习率调整、正则化策略和模型架构调整。
性能评估：在测试集上评估模型性能，确保模型满足预期的性能指标。
5. 模型部署和集成
部署策略：制定模型部署策略，包括部署环境、部署流程和监控机制。
系统集成：将模型集成到现有的业务系统中，确保模型的可用性和稳定性。
性能监控：监控模型在生产环境中的性能，包括准确率、响应时间和资源消耗。
6. 项目管理和协作
团队协作：与团队成员协作，包括数据科学家、工程师、产品经理和业务分析师。
进度管理：管理项目进度，确保项目按时完成。
沟通协调：与利益相关者沟通，协调资源，解决项目中的问题。
7. 文档和知识共享
文档编写：编写项目文档，包括设计文档、用户手册和操作手册。
知识共享：分享项目经验和知识，包括技术报告、博客文章和演讲。
8. 项目评估和迭代
项目评估：评估项目成果，包括业务影响、技术成果和团队表现。
项目迭代：根据项目评估结果，进行项目迭代和优化。
在回答时，可以根据你的实际经验进行调整和补充，确保回答真实可信。同时，可以强调你在项目中的领导力、创新能力和解决问题的能力。	回答样板仅供参考																							
36	如何防止模型在微调时遗忘？	防止大模型在微调时遗忘的方法有很多，以下是一些常见的策略：
1. 冻结部分参数
冻结模型的底层参数（如输入嵌入层等），只更新模型的上层参数。这种方法可以显著减少对旧任务对齐的破坏，同时保留模型对新任务的适应能力。例如，在Biography数据集上，冻结底层参数可将旧任务的准确率从11%提升至44%，而新任务的准确率仍可保持在99%以上。
2. 使用低秩适应（LoRA）
LoRA通过在特定层添加低秩矩阵，并只更新这些低秩矩阵，从而减少微调过程中的计算和存储成本。这种方法适用于资源受限的环境，同时也能有效减少遗忘。
3. 知识蒸馏
在微调过程中，将新任务学生模型的输出与原始任务教师模型的输出进行对比，从而引导模型保留原有任务的信息。这种方法可以帮助模型在适应新任务的同时，保留旧任务的知识。
4. 经验重放
将原始任务的数据和新任务的数据混合，共同训练模型。这种方法可以帮助模型同时学习新旧任务的知识，从而减少遗忘。
5. 弹性权重巩固（EWC）
通过对模型参数施加额外约束来保护那些对先前任务重要的权重。EWC的损失函数会惩罚对旧任务重要参数的大幅变化，从而减少遗忘。
6. 自我蒸馏
通过模型本身对任务数据进行生成引导，构建自我蒸馏数据集，减少任务数据信息分布与原始模型信息分布的差距。这种方法可以有效缓解微调后的灾难性遗忘。
7. 适配器（Adapters）
适配器是添加到现有Transformer架构中的前馈模块，用于减少全连接层之间的参数空间。这种方法可以在不改变原始模型结构的情况下，实现对新任务的快速适应。
8. 渐进式微调
使用小的学习率并进行多次微调，逐步调整模型参数，从而减少对旧知识的破坏。
9. 蒙特卡罗Dropout
在训练过程中使用Dropout可以帮助模型学习更具泛化性的特征，减少对特定数据的过度拟合。
10. 多任务学习
在微调时同时训练多个任务，使模型能够同时学习新旧任务的知识，从而减少遗忘。
通过结合以上方法，可以根据具体任务和模型的特点，选择合适的策略来有效防止大模型在微调时遗忘旧知识。																								
37	知道RAG的检索方法有几种吗？	
在 Retrieval-Augmented Generation (RAG) 中，检索方法主要可以分为以下几种：
1. 相似性检索 (Similarity Search)
定义：通过计算查询向量与存储向量的相似性得分，返回得分最高的记录。
常见相似性计算方法：
余弦相似性 (Cosine Similarity)：衡量两个向量之间的夹角余弦值，范围在 [−1,1] 之间，值越大表示越相似。
欧氏距离 (Euclidean Distance)：衡量两个向量之间的直线距离，距离越小表示越相似。
曼哈顿距离 (Manhattan Distance)：衡量两个向量在各维度上的绝对差之和，距离越小表示越相似。
2. 全文检索 (Full-Text Search)
定义：通过关键词构建倒排索引，在检索时通过关键词进行全文检索，找到对应的记录。
特点：
倒排索引：将文档中的每个单词映射到包含该单词的文档列表，从而实现快速检索。
关键词匹配：通过匹配用户查询中的关键词，找到相关的文档。
3. 混合检索 (Hybrid Search)
定义：结合传统的基于关键字的搜索（如 TF-IDF 或 BM25）和现代的语义或向量搜索，将两种方法的结果组合在一起。
实现方法：
Reciprocal Rank Fusion (RRF)：对不同检索方法的结果进行重新排序，以得到最终的输出结果。
Ensemble Retriever：将多个检索器（如基于 FAISS 的向量索引和基于 BM25 的检索器）结合起来，利用 RRF 算法进行结果的重排。
4. HyDE 方案 (HyDE: Hybrid Dense and Sparse Embeddings)
定义：结合稀疏和稠密嵌入的检索方法，通过双编码器模型生成稀疏和稠密嵌入，从而提升检索效果。
特点：
稀疏嵌入：利用传统的关键词匹配方法（如 TF-IDF 或 BM25）生成稀疏嵌入。
稠密嵌入：利用现代的语义嵌入方法生成稠密嵌入。
双编码器模型：将稀疏和稠密嵌入结合，生成更全面的检索结果。
5. 分层索引检索 (Hierarchical Indexing)
定义：通过构建分层的索引结构，逐步缩小检索范围，提高检索效率。
特点：
分层结构：将数据分层存储，先在高层进行粗粒度检索，再在低层进行细粒度检索。
高效检索：通过分层结构，减少每次检索的计算量，提高检索速度。
6. 查询重写 (Query Rewriting)
定义：通过修改和扩展用户查询，生成多个变体查询，以提高检索的召回率。
实现方法：
规则重写：使用预定义的规则和模式修改查询。
机器学习重写：训练模型学习如何根据示例转换查询。
混合重写：结合规则和机器学习方法。
7. 重排 (Re-ranking)
定义：在初步检索结果的基础上，通过额外的模型或算法对结果进行重新排序，以提高检索结果的相关性。
实现方法：
交叉编码器 (Cross-Encoder)：使用交叉编码器模型对初步检索结果进行重新排序。
基于元数据的重排：根据元数据信息对检索结果进行重排。
总结
RAG 的检索方法主要包括相似性检索、全文检索、混合检索、HyDE 方案、分层索引检索、查询重写和重排。这些方法各有优缺点，可以根据具体的应用场景和需求选择合适的检索方法，以提高检索效果和生成内容的质量。																								
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										
																										